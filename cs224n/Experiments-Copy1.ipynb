{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match-LSTM Layer\n",
    "\n",
    "- $W^q (l \\times l)$\n",
    "- $H^q (l \\times Q)$\n",
    "- $W^qH^q (l \\times Q)$\n",
    "\n",
    "- $h_{i-1}^r (l \\times 1)$\n",
    "- $W^rh_{i-1}^r (l \\times 1)$\n",
    "\n",
    "(Similarly for $W^ph_i^p$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import qa_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../data/squad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab, rev_vocab = qa_answer.initialize_vocab(os.path.join(path,'vocab.dat'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    with open(os.path.join(path, filename)) as f:\n",
    "        data = f.readlines()\n",
    "    \n",
    "    return [list(map(int,row.split())) for row in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_data, context_data, spans = [read_data(name) for name in ['train.ids.question', \n",
    "                                                                'train.ids.context',\n",
    "                                                                'train.span']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(81398, 81398, 81398)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_data), len(context_data), len(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incomplete and rough implementation of the model described in the Match-LSTM paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_var(name, shape, dtype):\n",
    "    return tf.get_variable(name=name, shape=shape, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'glove.trimmed.100.npz')) as f:\n",
    "    glove = np.load(f)['glove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove = np.float32(glove)\n",
    "glove.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_lengths = list(map(len,query_data))\n",
    "context_lengths = list(map(len,context_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_units = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use the final states of the question LSTM as the initial states of the context LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tile_batch(weight,num,rank):\n",
    "    for r in range(rank-len(weight.shape)):\n",
    "        weight = tf.expand_dims(weight,axis=0)\n",
    "    return tf.tile(weight, [num]+[1]*(len(weight.shape)-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shape(tensor):\n",
    "    return tensor.get_shape().as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_lstm(ids, lengths, num_units, init_stat_fw = None, init_stat_bw = None):\n",
    "    embeds = tf.nn.embedding_lookup(glove, ids)\n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(num_units = num_units)\n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(num_units = num_units)\n",
    "    out, state = tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, \n",
    "                                                inputs=embeds, \n",
    "                                                sequence_length=lengths,\n",
    "                                                initial_state_fw = init_stat_fw,\n",
    "                                                initial_state_bw = init_stat_bw,\n",
    "                                                dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.variable_scope('placeholders'):\n",
    "    qids = tf.placeholder(name = 'q_ids', shape=[128,None], dtype = tf.int32)\n",
    "    cids = tf.placeholder(name = 'c_ids', shape=[128,None], dtype = tf.int32)\n",
    "    qlen = tf.placeholder(name = 'q_len', shape=[128], dtype = tf.int32)\n",
    "    clen = tf.placeholder(name = 'c_len', shape=[128], dtype = tf.int32)\n",
    "    \n",
    "with tf.variable_scope('q_reps'):\n",
    "    question_embeds, last_states = run_lstm(qids, qlen, lstm_units)\n",
    "    question_reps = tf.concat(question_embeds, 2)\n",
    "    \n",
    "with tf.variable_scope('c_reps'):\n",
    "    fw_stat, bw_stat = last_states\n",
    "    context_embeds, _ = run_lstm(cids, clen, lstm_units, fw_stat, bw_stat)\n",
    "    context_reps = tf.concat(context_embeds, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec_cell = self.cell(self.enc_emb_size)\n",
    "\n",
    "attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n",
    "    num_units=64,\n",
    "    memory=question_reps,\n",
    "    name='BahdanauAttention')\n",
    "\n",
    "dec_cell = tf.contrib.seq2seq.AttentionWrapper(\n",
    "    cell=dec_cell,\n",
    "    attention_mechanism=attn_mech,\n",
    "    name='Attention_Wrapper')\n",
    "\n",
    "\n",
    "match_inputs = tf.concat()\n",
    "\n",
    "match_out, match_state = run_lstm(ids, lengths, num_units, init_stat_fw = None, init_stat_bw = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MatchLSTM(object):\n",
    "    def __init__(self, H_p, H_q, P, Q):\n",
    "        wnames = ['W_'+i for i in 'pqr']\n",
    "        self.weights = {}\n",
    "        self.biases = {'b_p':make_var('b_p', shape=[2*lstm_units], dtype=tf.float32)}\n",
    "        self.biases['b'] = make_var('b',shape=[1], dtype=tf.float32)\n",
    "        self.embeds = dict(zip(['H_'+i for i in 'pq'], [H_p, H_q]))\n",
    "            \n",
    "        self.weights['w'] = make_var('w', shape=[2*lstm_units], dtype=tf.float32)\n",
    "        \n",
    "        for wname in wnames:\n",
    "            self.weights[wname] = make_var(wname, shape=[2*lstm_units,2*lstm_units], dtype=tf.float32)\n",
    "        \n",
    "        self.reused_term = tf.matmul(H_q, tile_batch(self.weights['W_q'],128,3))\n",
    "        self.P = P\n",
    "        self.Q = Q\n",
    "        self.cell_fw = tf.nn.rnn_cell.LSTMCell(2*lstm_units)\n",
    "        self.cell_bw = tf.nn.rnn_cell.LSTMCell(2*lstm_units)\n",
    "            \n",
    "    def calculate_attention(self,h_r,i):\n",
    "        temporal_term = tf.matmul(self.embeds['H_p'][:,i:i+1], tile_batch(self.weights['W_p'],128,3),name='h_pW_p') +\\\n",
    "            tf.matmul(h_r, tile_batch(self.weights['W_r'],128,3), name ='h_rW_r') +\\\n",
    "            tile_batch(self.biases['b_p'],128,3)\n",
    "        temporal_term_tiled = tf.tile(\n",
    "            temporal_term, [1,self.Q,1],\n",
    "            )\n",
    "        G = tf.tanh(self.reused_term + temporal_term_tiled)\n",
    "        w_expanded = tf.expand_dims(self.weights['w'],axis=1)\n",
    "        G_times_w = tf.matmul(G, tile_batch(w_expanded,128,3),name='Gw') \n",
    "        alpha = tf.nn.softmax(G_times_w+ tf.tile(tile_batch(self.biases['b'],128,3), [1,self.Q,1]))               \n",
    "        return alpha\n",
    "\n",
    "    def make_rnn_inputs(self, h_r, i):\n",
    "        alpha = self.calculate_attention(h_r,i)\n",
    "        z = tf.concat([self.embeds['H_p'][:,i:i+1],tf.matmul(alpha, self.embeds['H_q'], transpose_a=True)],2)\n",
    "        return z\n",
    "    \n",
    "    def run_cell(self, i,state=None,bw=False):\n",
    "        cell = self.cell_bw if bw else self.cell_fw\n",
    "        state = cell.zero_state(128,dtype=tf.float32) if state is None else state\n",
    "        index = self.P - 1 - i if bw else i \n",
    "        inputs = self.make_rnn_inputs(tf.expand_dims(state.h,axis=1), index)\n",
    "        _, state = cell(tf.squeeze(inputs, axis=1),state)\n",
    "        return state\n",
    "\n",
    "    def run(self):\n",
    "        H_fw = []\n",
    "        H_bw = []\n",
    "        for i in range(self.P):\n",
    "            state_fw = run_cell(i, state_fw if i else None)\n",
    "            state_bw = run_cell(i, state_bw if i else None,True)\n",
    "            H_fw.append(state_fw.h)\n",
    "            H_bw.append(state_bw.h)\n",
    "        H_fw = tf.concat(H_fw, 1)\n",
    "        H_bw = tf.concat(H_bw, 1)\n",
    "        self.H_r = tf.concat( [H_fw, H_bw], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml = MatchLSTM(context_reps, question_reps, 100, 100)\n",
    "cell = tf.nn.rnn_cell.LSTMCell(2*lstm_units)\n",
    "h_fw_r = cell.zero_state(128,dtype=tf.float32).h\n",
    "cell.zero_state(128,dtype=tf.float32).h.shape\n",
    "ri = ml.calculate_attention(tf.expand_dims(h_fw_r,axis=1),0)\n",
    "ri.get_shape().as_list()\n",
    "a = ml.make_rnn_inputs( tf.expand_dims(h_fw_r,axis=1), 0)\n",
    "b = ml.run_cell(1,cell.zero_state(128,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class AnswerPointer(object):\n",
    "    def __init__(self, H_r, P):\n",
    "        self.weights = self._make_vars(['V','W_a','v'],[[2*lstm_units]*2]*2+[2*lstm_units])\n",
    "        self.biases = self._make_vars(['b_a','c'],[[2*lstm_units],[1]])\n",
    "        self.H_tilde_r = tf.concat( [H_r,tf.zeros_like(H_r[:,0:1,:])],1) \n",
    "        self.H_r = H_r\n",
    "        self.reused_term = tf.matmul(self.H_tilde_r, tile_batch(self.weights['V'],128,3))\n",
    "        self.P = P\n",
    "        with tf.variable_scope('ap'):\n",
    "            self.cell = tf.nn.rnn_cell.LSTMCell(2*lstm_units)\n",
    "    \n",
    "    def _make_vars(self, names, shapes, dtype=tf.float32):\n",
    "        return dict(zip(names,[make_var(name, shape, dtype) for name,shape in zip(names,shapes)]))\n",
    "    \n",
    "    def calculate_attention(self,h_a,i):\n",
    "        temporal_term = tf.matmul(h_a, tile_batch(self.weights['W_a'],128,3), name ='h_aW_a') +\\\n",
    "            tile_batch(self.biases['b_a'],128,3)\n",
    "        temporal_term_tiled = tf.tile(\n",
    "            temporal_term, [1,self.P+1,1],\n",
    "            )\n",
    "        F = tf.tanh(self.reused_term + temporal_term_tiled)\n",
    "        v_expanded = tf.expand_dims(self.weights['v'],axis=1)\n",
    "        F_times_v = tf.matmul(F, tile_batch(v_expanded,128,3),name='Fv') \n",
    "        beta = tf.nn.softmax(F_times_v+ tf.tile(tile_batch(self.biases['c'],128,3), [1,self.P + 1,1]))               \n",
    "        return beta\n",
    "\n",
    "    def make_rnn_inputs(self, h_a, i):\n",
    "        beta = self.calculate_attention(h_a,i)\n",
    "        z = tf.matmul(beta, self.H_tilde_r, transpose_a=True)\n",
    "        return z, beta\n",
    "    \n",
    "    def run_cell(self, i,state=None):\n",
    "        cell = self.cell\n",
    "        state = cell.zero_state(128,dtype=tf.float32) if state is None else state\n",
    "        inputs, beta = self.make_rnn_inputs(tf.expand_dims(state.h,axis=1), i)\n",
    "        with tf.variable_scope('ap'):\n",
    "            _, state = cell(tf.squeeze(inputs, axis=1),state)\n",
    "        return state, beta\n",
    "        \n",
    "    def _make_mask(self,beta):\n",
    "        return 1-tf.cast(tf.equal(tf.argmax(beta,axis=2), self.P),tf.float32)\n",
    "    \n",
    "    def _make_index(self,positions):\n",
    "        colm1 = tf.range(0,128)\n",
    "        colm2 = tf.zeros_like(colm1)\n",
    "        colm3 = positions\n",
    "        return tf.transpose(tf.stack([colm1,colm2,colm3]))\n",
    "    \n",
    "    def run_step(self,k,state=None):\n",
    "        state, beta = self.run_cell(k, state)\n",
    "        probs = tf.gather_nd(beta, self._make_index(self.answers[k]))\n",
    "        return state,tf.log(probs)\n",
    "\n",
    "    def run(self):\n",
    "        self.all_probs = []\n",
    "        for k in range(self.max_answer_length):\n",
    "            state,logprobs = self.run_step(k,state if k else None)\n",
    "            #mask = self.mask if train else _make_mask i.e. infer it has stopped\n",
    "            self.all_probs.append(logprobs)\n",
    "#         self.loss = -tf.reduce_sum(self.all_probs*self.mask if train else self.all_probs,axis=0)\n",
    "        self.all_probs = tf.transpose(tf.stack\n",
    "        self.loss = -tf.reduce_sum(self.all_probs*self.mask,axis=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = tf.stack([b.h]*100,name='hi090')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B = tf.reshape(B,[128,-1,600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(128), Dimension(100), Dimension(600)])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AP = AnswerPointer(B,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose:0' shape=(128, 3) dtype=int32>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP._make_index(np.arange(128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'sub:0' shape=(128, 100) dtype=float32>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP._make_mask(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AP.answers = tf.placeholder(name='answers',shape=[128,None],dtype=tf.int32)\n",
    "AP.mask = tf.placeholder(name='mask',shape=[128,None],dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = AP.run_step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lstm_cell_8'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP.cell.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "AP.max_answer_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 100 and 128 for 'mul' (op: 'Mul') with input shapes: [100,128], [128,?].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-610800f66538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-87-412dacb5e641>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_probs\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    852\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m   \u001b[0;31m# Propagate func.__doc__ to the wrappers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1059\u001b[0m   \u001b[0mis_tensor_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1060\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mis_tensor_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1061\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1062\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Case: Dense * Sparse.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_mul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1375\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mHas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m`\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m   \"\"\"\n\u001b[0;32m-> 1377\u001b[0;31m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_op_def_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    765\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    766\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2506\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2507\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2508\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2509\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2510\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1871\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1872\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1873\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1874\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1875\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/user/anaconda/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 100 and 128 for 'mul' (op: 'Mul') with input shapes: [100,128], [128,?]."
     ]
    }
   ],
   "source": [
    "AP.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(128), Dimension(128)])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AP.loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128, 100, 1]"
      ]
     },
     "execution_count": 581,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(128), Dimension(1), Dimension(1200)])"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(128), Dimension(600)])"
      ]
     },
     "execution_count": 585,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialise H_r to some state e.g. random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.batch_matmul?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "H_r = tf.concat([H_fw, H_bw], axis=0)\n",
    "H_tilde_r = tf.concat({H_r,tf.zeros((1,P))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shared_state = tf.matmul(V,H_tilde_r)\n",
    "def pointer(h_a):\n",
    "    temporal_state = tf.matmul(W_a,h_a) + b_\n",
    "    temporal_state_tiled = tf.tile(temporal_state(\n",
    "            [1, P+1]\n",
    "        ))\n",
    "    F = tf.tanh(shared_state + temporal_state_tiled)\n",
    "    beta = tf.nn.softmax(\n",
    "        tf.matmul(tf.transpose(v, F) + tf.tile(c,[P+1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_probs = []\n",
    "for k in range(answer_length):\n",
    "    beta = pointer(h_a)\n",
    "    h_a = cell(tf.matmul(H_tilde_r,tf.transpose(beta)),\n",
    "                h_a)\n",
    "    seq_probs.append(beta[answer[k]])\n",
    "    #End token - (P+1)th element whose index is P\n",
    "    if tf.argmax(beta) == P:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_loss = -tf.reduce_sum(tf.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boundary_probs = []\n",
    "for n in range(2):\n",
    "    beta = pointer(h_a)\n",
    "    h_a = cell(tf.matmul(H_tilde_r,tf.transpose(beta)),\n",
    "                h_a)\n",
    "    probs.append(beta[position[n]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "boundary_loss = -tf.reduce_sum(tf.log(boundary_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
