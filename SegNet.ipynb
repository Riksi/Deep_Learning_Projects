{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_bn_relu(x, filters, is_train, relu=True):\n",
    "    #print('conv_bn_relu')\n",
    "    #print(x.shape)\n",
    "    x = tf.layers.conv2d(x, use_bias=False, filters=filters, kernel_size=3,padding='SAME')\n",
    "    x = tf.layers.batch_normalization(x, training=is_train)\n",
    "    if relu:\n",
    "        x = tf.nn.relu(x)\n",
    "    #print(x.shape)\n",
    "    #print()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool(x):\n",
    "    #print('pool')\n",
    "    #print(x.shape)\n",
    "    shape = tf.shape(x)\n",
    "    x, pool_inds = tf.nn.max_pool_with_argmax(x, ksize=[1,2,2,1],strides=[1,2,2,1], padding='VALID')\n",
    "    pool_inds = tf.cast(pool_inds, tf.int32)\n",
    "    #print(x.shape)\n",
    "    #print()\n",
    "    return x, pool_inds, shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpool(x, pool_inds, shape):\n",
    "    #print('unpool')\n",
    "    #print(x.shape)\n",
    "    inds_flat = tf.reshape(pool_inds,[-1,1])\n",
    "    batch_range = tf.range(start=0,limit=tf.shape(x)[0],delta=1)\n",
    "    batch_inds = tf.tile(tf.expand_dims(batch_range, axis=-1), [1,tf.size(pool_inds[0])])\n",
    "    batch_inds_flat = tf.reshape(batch_inds, [-1,1])\n",
    "    inds = tf.concat([batch_inds_flat, inds_flat], axis=1)\n",
    "    x_flat = tf.reshape(x,[-1])\n",
    "    shape_flat = [shape[0], tf.reduce_prod(shape[1:])]\n",
    "    result = tf.scatter_nd(indices=inds, updates=x_flat, shape=shape_flat)\n",
    "    result = tf.reshape(result, shape)\n",
    "    #print(result.shape)\n",
    "    #print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def pool(x):\n",
    "    shape = x.get_shape().as_list()\n",
    "    x, pool_inds = tf.nn.max_pool_with_argmax(x, ksize=[1,2,2,1],strides=[1,2,2,1], padding='VALID')\n",
    "    pool_inds = tf.cast(pool_inds, tf.int32)\n",
    "    return x, pool_inds, shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def unpool(x, pool_inds, shape):\n",
    "    unp =  tf.layers.conv2d_transpose(x, strides=2, kernel_size=2, padding='SAME',\n",
    "                                      filters=x.get_shape().as_list()[-1])\n",
    "    pad1 = shape[1] - unp.get_shape().as_list()[1]\n",
    "    pad2 = shape[2] - unp.get_shape().as_list()[2]\n",
    "    paddings = [[0,0],[math.floor(pad1/2), math.ceil(pad1/2)]\n",
    "                ,[math.floor(pad2/2), math.ceil(pad2/2)],[0,0]]\n",
    "    unp = tf.pad(unp, paddings=paddings)\n",
    "    return unp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def segnet(x, n_classes, is_train):\n",
    "    conv1 = conv_bn_relu(x, 64, is_train)\n",
    "    conv2 = conv_bn_relu(conv1, 64, is_train)\n",
    "    pool1, inds1, shape1 = pool(conv2)\n",
    "    \n",
    "    conv3 = conv_bn_relu(pool1, 128, is_train)\n",
    "    conv4 = conv_bn_relu(conv3, 128, is_train)\n",
    "    pool2, inds2, shape2 = pool(conv4)\n",
    "\n",
    "    conv5 = conv_bn_relu(pool2, 256, is_train)\n",
    "    conv6 = conv_bn_relu(conv5, 256, is_train)\n",
    "    conv7 = conv_bn_relu(conv6, 256, is_train)\n",
    "    pool3, inds3, shape3 = pool(conv7)\n",
    "\n",
    "    conv8 = conv_bn_relu(pool3, 512, is_train)\n",
    "    conv9 = conv_bn_relu(conv8, 512, is_train)\n",
    "    conv10 = conv_bn_relu(conv9, 512, is_train)\n",
    "    pool4, inds4, shape4 = pool(conv10)\n",
    "\n",
    "    conv11 = conv_bn_relu(pool4, 512, is_train)\n",
    "    conv12 = conv_bn_relu(conv11, 512, is_train)\n",
    "    conv13 = conv_bn_relu(conv12, 512, is_train)\n",
    "    pool5, inds5, shape5 = pool(conv13)\n",
    "\n",
    "    unpool1 = unpool(pool5, inds5, shape5)\n",
    "    upconv1 = conv_bn_relu(unpool1, 512, is_train, relu=False)\n",
    "    upconv2 = conv_bn_relu(upconv1, 512, is_train, relu=False)\n",
    "    upconv3 = conv_bn_relu(upconv2, 512, is_train, relu=False)\n",
    "\n",
    "    unpool2 = unpool(upconv3, inds4, shape4)\n",
    "    upconv4 = conv_bn_relu(unpool2, 512, is_train, relu=False)\n",
    "    upconv5 = conv_bn_relu(upconv4, 512, is_train, relu=False)\n",
    "    upconv6 = conv_bn_relu(upconv5, 256, is_train, relu=False)\n",
    "\n",
    "    unpool3 = unpool(upconv6, inds3, shape3)\n",
    "    upconv7 = conv_bn_relu(unpool3, 256, is_train, relu=False)\n",
    "    upconv8 = conv_bn_relu(upconv7, 256, is_train, relu=False)\n",
    "    upconv9 = conv_bn_relu(upconv8, 128, is_train, relu=False)\n",
    "\n",
    "    unpool4 = unpool(upconv9, inds2, shape2)\n",
    "    upconv10 = conv_bn_relu(unpool4, 128, is_train, relu=False)\n",
    "    upconv11 = conv_bn_relu(upconv10, 64, is_train, relu=False)\n",
    "\n",
    "    unpool5 = unpool(upconv11, inds1, shape1)\n",
    "    upconv12 = conv_bn_relu(unpool5, 64, is_train, relu=False)\n",
    "    #upconv13 = conv_bn_relu(upconv12, 64)\n",
    "\n",
    "    logits = conv_bn_relu(upconv12, n_classes, is_train, relu=False)\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_files = open('SegNet/CamVid/train.txt').read().replace('/SegNet','SegNet').split('\\n')[:-1]\n",
    "valid_files = open('SegNet/CamVid/val.txt').read().replace('/SegNet','SegNet').split('\\n')[:-1]\n",
    "img_files, mask_files = zip(*[i.split() for i in train_files[:360]])\n",
    "valid_img_files, valid_mask_files = zip(*[i.split() for i in valid_files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downsample = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 360/2\n",
    "width = 480/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gen(batch_size, img_files, mask_files, ds=1):\n",
    "    num_ex = len(img_files)\n",
    "    shuffle = np.random.permutation(num_ex)\n",
    "    batches_per_epoch = math.ceil(num_ex/batch_size)\n",
    "    img_files = np.array(img_files)[shuffle]\n",
    "    mask_files = np.array(mask_files)[shuffle]\n",
    "    for i in range(batches_per_epoch):\n",
    "        slc = slice(i*batch_size, (i+1)*batch_size)\n",
    "        files = [img_files[slc], mask_files[slc]]\n",
    "        imgs, masks = [np.array([plt.imread(j) for j in f])[:,::ds,::ds] \n",
    "                       for f in files]\n",
    "        yield (i+1, imgs.astype(np.float32), (255*masks).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = np.zeros(12)\n",
    "tg = batch_gen(1,img_files, mask_files)\n",
    "eq = np.reshape(np.arange(12), [1,12])\n",
    "for i,im,mk in tg:\n",
    "    counts += np.sum(np.reshape(mk,[-1,1])==eq, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.24624816,   0.17903213,   4.28582449,   0.13155787,\n",
       "         0.94207353,   0.42706214,   3.51309235,   3.7142853 ,\n",
       "         0.69747728,   6.72559599,  14.52094855,   1.06551678])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.median(counts)/counts\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(name='x',dtype=tf.float32,shape=[None,height,width,3])\n",
    "y = tf.placeholder(name='y',dtype=tf.int32,shape=[None,height,width])\n",
    "is_train = tf.placeholder(name='is_train',dtype=tf.bool,shape=None)\n",
    "\n",
    "n_classes = 12\n",
    "weights = np.ones(n_classes)/n_classes\n",
    "logits = segnet(x,n_classes,is_train)\n",
    "preds = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "\n",
    "y_one_hot = tf.one_hot(tf.cast(y, tf.int32), depth=n_classes, axis=-1)\n",
    "preds_one_hot = tf.one_hot(preds, depth=n_classes, axis=-1)\n",
    "\n",
    "\n",
    "class_counts = tf.reduce_sum(y_one_hot, axis=[0,1,2])\n",
    "equal = tf.cast(tf.equal(y, preds), tf.float32)\n",
    "num_right = tf.reduce_sum(tf.expand_dims(equal,axis=-1)*y_one_hot, axis=[0,1,2])\n",
    "class_acc = num_right/class_counts\n",
    "\n",
    "macc = tf.reduce_mean(equal)\n",
    "\n",
    "intersection = tf.reduce_sum(y_one_hot*preds_one_hot, axis=[1,2])\n",
    "union = tf.reduce_sum(y_one_hot, axis=[1,2]) + tf.reduce_sum(preds_one_hot, axis=[1,2]) - intersection\n",
    "iou = tf.reduce_mean((intersection + 1e-10)/(union + 1e-10), axis=0)\n",
    "miou = tf.reduce_mean(iou)\n",
    "\n",
    "\n",
    "logprobs = tf.nn.log_softmax(logits)\n",
    "losses = -tf.reduce_mean(logprobs*y_one_hot, axis=(0,1,2))\n",
    "loss = tf.reduce_sum(losses*weights)\n",
    "\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(update_ops):\n",
    "    opt = tf.train.MomentumOptimizer(1e-1,momentum=0.9)\n",
    "    train_step = opt.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 70\n",
    "batch_size = 10\n",
    "num_ex = len(img_files)\n",
    "num_valid = len(valid_img_files)\n",
    "batches_per_epoch = math.ceil(num_ex/batch_size)\n",
    "num_valid_batches = math.ceil(num_valid/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Training: epoch 1/70, batch 36/36, loss:0.1921, iou:0.0763, acc:0.2739\n",
      "Validation: batch 11/11, loss: 0.1883, iou: 0.0240, acc:0.2881\n",
      "Training: epoch 2/70, batch 36/36, loss:0.1123, iou:0.1905, acc:0.6695\n",
      "Validation: batch 11/11, loss: 0.1709, iou: 0.0252, acc:0.2915\n",
      "Training: epoch 3/70, batch 36/36, loss:0.0928, iou:0.2666, acc:0.6932\n",
      "Validation: batch 11/11, loss: 0.1662, iou: 0.0317, acc:0.3106\n",
      "Training: epoch 4/70, batch 36/36, loss:0.0811, iou:0.2870, acc:0.7216\n",
      "Validation: batch 11/11, loss: 0.1612, iou: 0.0371, acc:0.3288\n",
      "Training: epoch 5/70, batch 36/36, loss:0.0713, iou:0.3303, acc:0.7703\n",
      "Validation: batch 11/11, loss: 0.1588, iou: 0.0426, acc:0.3488\n",
      "Training: epoch 6/70, batch 36/36, loss:0.0652, iou:0.3477, acc:0.7884\n",
      "Validation: batch 11/11, loss: 0.1600, iou: 0.0534, acc:0.3658\n",
      "Training: epoch 7/70, batch 36/36, loss:0.0599, iou:0.3578, acc:0.8007\n",
      "Validation: batch 11/11, loss: 0.1532, iou: 0.0668, acc:0.4025\n",
      "Training: epoch 8/70, batch 36/36, loss:0.0552, iou:0.3734, acc:0.8138\n",
      "Validation: batch 11/11, loss: 0.1503, iou: 0.0730, acc:0.4105\n",
      "Training: epoch 9/70, batch 36/36, loss:0.0513, iou:0.3978, acc:0.8298\n",
      "Validation: batch 11/11, loss: 0.1372, iou: 0.1074, acc:0.4533\n",
      "Training: epoch 10/70, batch 36/36, loss:0.0481, iou:0.4110, acc:0.8389\n",
      "Validation: batch 11/11, loss: 0.1320, iou: 0.1312, acc:0.5132\n",
      "Training: epoch 11/70, batch 36/36, loss:0.0456, iou:0.4209, acc:0.8450\n",
      "Validation: batch 11/11, loss: 0.1153, iou: 0.1620, acc:0.5925\n",
      "Training: epoch 12/70, batch 36/36, loss:0.0427, iou:0.4349, acc:0.8566\n",
      "Validation: batch 11/11, loss: 0.1109, iou: 0.1741, acc:0.5910\n",
      "Training: epoch 13/70, batch 36/36, loss:0.0403, iou:0.4482, acc:0.8662\n",
      "Validation: batch 11/11, loss: 0.1040, iou: 0.1813, acc:0.6092\n",
      "Training: epoch 14/70, batch 36/36, loss:0.0394, iou:0.4544, acc:0.8699\n",
      "Validation: batch 11/11, loss: 0.0733, iou: 0.2604, acc:0.7152\n",
      "Training: epoch 15/70, batch 36/36, loss:0.0380, iou:0.4627, acc:0.8747\n",
      "Validation: batch 11/11, loss: 0.0909, iou: 0.2176, acc:0.6407\n",
      "Training: epoch 16/70, batch 36/36, loss:0.0371, iou:0.4628, acc:0.8774\n",
      "Validation: batch 11/11, loss: 0.0618, iou: 0.3141, acc:0.7747\n",
      "Training: epoch 17/70, batch 36/36, loss:0.0354, iou:0.4727, acc:0.8826\n",
      "Validation: batch 11/11, loss: 0.0478, iou: 0.3718, acc:0.8464\n",
      "Training: epoch 18/70, batch 36/36, loss:0.0342, iou:0.4751, acc:0.8869\n",
      "Validation: batch 11/11, loss: 0.0536, iou: 0.3566, acc:0.8184\n",
      "Training: epoch 19/70, batch 36/36, loss:0.0340, iou:0.4769, acc:0.8865\n",
      "Validation: batch 11/11, loss: 0.0522, iou: 0.3657, acc:0.8293\n",
      "Training: epoch 20/70, batch 36/36, loss:0.0329, iou:0.4796, acc:0.8900\n",
      "Validation: batch 11/11, loss: 0.0472, iou: 0.3793, acc:0.8435\n",
      "Training: epoch 21/70, batch 36/36, loss:0.0325, iou:0.4763, acc:0.8907\n",
      "Validation: batch 11/11, loss: 0.0449, iou: 0.3807, acc:0.8487\n",
      "Training: epoch 22/70, batch 36/36, loss:0.0316, iou:0.4809, acc:0.8936\n",
      "Validation: batch 11/11, loss: 0.0439, iou: 0.3864, acc:0.8521\n",
      "Training: epoch 23/70, batch 36/36, loss:0.0309, iou:0.4861, acc:0.8953\n",
      "Validation: batch 11/11, loss: 0.0441, iou: 0.3864, acc:0.8534\n",
      "Training: epoch 24/70, batch 36/36, loss:0.0304, iou:0.4830, acc:0.8974\n",
      "Validation: batch 11/11, loss: 0.0429, iou: 0.3921, acc:0.8549\n",
      "Training: epoch 25/70, batch 36/36, loss:0.0293, iou:0.4916, acc:0.9013\n",
      "Validation: batch 11/11, loss: 0.0437, iou: 0.3995, acc:0.8557\n",
      "Training: epoch 26/70, batch 36/36, loss:0.0283, iou:0.4961, acc:0.9045\n",
      "Validation: batch 11/11, loss: 0.0440, iou: 0.4023, acc:0.8555\n",
      "Training: epoch 27/70, batch 36/36, loss:0.0280, iou:0.5006, acc:0.9054\n",
      "Validation: batch 11/11, loss: 0.0440, iou: 0.4004, acc:0.8540\n",
      "Training: epoch 28/70, batch 36/36, loss:0.0272, iou:0.5040, acc:0.9081\n",
      "Validation: batch 11/11, loss: 0.0435, iou: 0.4118, acc:0.8592\n",
      "Training: epoch 29/70, batch 36/36, loss:0.0269, iou:0.5063, acc:0.9092\n",
      "Validation: batch 11/11, loss: 0.0431, iou: 0.4122, acc:0.8600\n",
      "Training: epoch 30/70, batch 36/36, loss:0.0265, iou:0.5122, acc:0.9107\n",
      "Validation: batch 11/11, loss: 0.0430, iou: 0.4146, acc:0.8605\n",
      "Training: epoch 31/70, batch 36/36, loss:0.0258, iou:0.5165, acc:0.9130\n",
      "Validation: batch 11/11, loss: 0.0409, iou: 0.4143, acc:0.8655\n",
      "Training: epoch 32/70, batch 36/36, loss:0.0259, iou:0.5131, acc:0.9128\n",
      "Validation: batch 11/11, loss: 0.0408, iou: 0.4139, acc:0.8663\n",
      "Training: epoch 33/70, batch 36/36, loss:0.0252, iou:0.5209, acc:0.9151\n",
      "Validation: batch 11/11, loss: 0.0404, iou: 0.4056, acc:0.8651\n",
      "Training: epoch 34/70, batch 36/36, loss:0.0247, iou:0.5262, acc:0.9164\n",
      "Validation: batch 11/11, loss: 0.0403, iou: 0.4239, acc:0.8717\n",
      "Training: epoch 35/70, batch 36/36, loss:0.0239, iou:0.5344, acc:0.9193\n",
      "Validation: batch 11/11, loss: 0.0409, iou: 0.4207, acc:0.8657\n",
      "Training: epoch 36/70, batch 36/36, loss:0.0235, iou:0.5340, acc:0.9201\n",
      "Validation: batch 11/11, loss: 0.0395, iou: 0.4210, acc:0.8690\n",
      "Training: epoch 37/70, batch 36/36, loss:0.0230, iou:0.5378, acc:0.9218\n",
      "Validation: batch 11/11, loss: 0.0394, iou: 0.4175, acc:0.8677\n",
      "Training: epoch 38/70, batch 36/36, loss:0.0226, iou:0.5421, acc:0.9230\n",
      "Validation: batch 11/11, loss: 0.0423, iou: 0.4269, acc:0.8643\n",
      "Training: epoch 39/70, batch 36/36, loss:0.0225, iou:0.5442, acc:0.9232\n",
      "Validation: batch 11/11, loss: 0.0417, iou: 0.4274, acc:0.8656\n",
      "Training: epoch 40/70, batch 36/36, loss:0.0223, iou:0.5457, acc:0.9238\n",
      "Validation: batch 11/11, loss: 0.0404, iou: 0.4266, acc:0.8708\n",
      "Training: epoch 41/70, batch 36/36, loss:0.0217, iou:0.5529, acc:0.9259\n",
      "Validation: batch 11/11, loss: 0.0401, iou: 0.4258, acc:0.8691\n",
      "Training: epoch 42/70, batch 36/36, loss:0.0216, iou:0.5528, acc:0.9260\n",
      "Validation: batch 11/11, loss: 0.0387, iou: 0.4346, acc:0.8746\n",
      "Training: epoch 43/70, batch 36/36, loss:0.0212, iou:0.5587, acc:0.9269\n",
      "Validation: batch 11/11, loss: 0.0381, iou: 0.4358, acc:0.8738\n",
      "Training: epoch 44/70, batch 36/36, loss:0.0209, iou:0.5607, acc:0.9281\n",
      "Validation: batch 11/11, loss: 0.0395, iou: 0.4355, acc:0.8699\n",
      "Training: epoch 45/70, batch 36/36, loss:0.0206, iou:0.5643, acc:0.9290\n",
      "Validation: batch 11/11, loss: 0.0383, iou: 0.4406, acc:0.8735\n",
      "Training: epoch 46/70, batch 36/36, loss:0.0204, iou:0.5642, acc:0.9297\n",
      "Validation: batch 11/11, loss: 0.0389, iou: 0.4414, acc:0.8736\n",
      "Training: epoch 47/70, batch 36/36, loss:0.0202, iou:0.5656, acc:0.9302\n",
      "Validation: batch 11/11, loss: 0.0400, iou: 0.4464, acc:0.8716\n",
      "Training: epoch 48/70, batch 36/36, loss:0.0199, iou:0.5690, acc:0.9308\n",
      "Validation: batch 11/11, loss: 0.0389, iou: 0.4423, acc:0.8732\n",
      "Training: epoch 49/70, batch 36/36, loss:0.0200, iou:0.5701, acc:0.9307\n",
      "Validation: batch 11/11, loss: 0.0402, iou: 0.4476, acc:0.8698\n",
      "Training: epoch 50/70, batch 36/36, loss:0.0193, iou:0.5757, acc:0.9328\n",
      "Validation: batch 11/11, loss: 0.0386, iou: 0.4463, acc:0.8729\n",
      "Training: epoch 51/70, batch 36/36, loss:0.0190, iou:0.5820, acc:0.9335\n",
      "Validation: batch 11/11, loss: 0.0382, iou: 0.4385, acc:0.8724\n",
      "Training: epoch 52/70, batch 36/36, loss:0.0194, iou:0.5757, acc:0.9324\n",
      "Validation: batch 11/11, loss: 0.0397, iou: 0.4424, acc:0.8712\n",
      "Training: epoch 53/70, batch 36/36, loss:0.0193, iou:0.5776, acc:0.9325\n",
      "Validation: batch 11/11, loss: 0.0389, iou: 0.4470, acc:0.8748\n",
      "Training: epoch 54/70, batch 36/36, loss:0.0187, iou:0.5835, acc:0.9346\n",
      "Validation: batch 11/11, loss: 0.0388, iou: 0.4429, acc:0.8712\n",
      "Training: epoch 55/70, batch 36/36, loss:0.0184, iou:0.5887, acc:0.9354\n",
      "Validation: batch 11/11, loss: 0.0391, iou: 0.4548, acc:0.8724\n",
      "Training: epoch 56/70, batch 36/36, loss:0.0182, iou:0.5893, acc:0.9361\n",
      "Validation: batch 11/11, loss: 0.0378, iou: 0.4476, acc:0.8755\n",
      "Training: epoch 57/70, batch 36/36, loss:0.0181, iou:0.5906, acc:0.9364\n",
      "Validation: batch 11/11, loss: 0.0373, iou: 0.4424, acc:0.8752\n",
      "Training: epoch 58/70, batch 36/36, loss:0.0176, iou:0.5989, acc:0.9379\n",
      "Validation: batch 11/11, loss: 0.0384, iou: 0.4457, acc:0.8724\n",
      "Training: epoch 59/70, batch 36/36, loss:0.0174, iou:0.5966, acc:0.9382\n",
      "Validation: batch 11/11, loss: 0.0389, iou: 0.4476, acc:0.8721\n",
      "Training: epoch 60/70, batch 36/36, loss:0.0173, iou:0.5979, acc:0.9384\n",
      "Validation: batch 11/11, loss: 0.0385, iou: 0.4473, acc:0.8710\n",
      "Training: epoch 61/70, batch 36/36, loss:0.0171, iou:0.6063, acc:0.9392\n",
      "Validation: batch 11/11, loss: 0.0382, iou: 0.4499, acc:0.8725\n",
      "Training: epoch 62/70, batch 36/36, loss:0.0171, iou:0.6033, acc:0.9393\n",
      "Validation: batch 11/11, loss: 0.0391, iou: 0.4529, acc:0.8725\n",
      "Training: epoch 63/70, batch 36/36, loss:0.0170, iou:0.6033, acc:0.9394\n",
      "Validation: batch 11/11, loss: 0.0379, iou: 0.4399, acc:0.8718\n",
      "Training: epoch 64/70, batch 36/36, loss:0.0168, iou:0.6059, acc:0.9399\n",
      "Validation: batch 11/11, loss: 0.0383, iou: 0.4394, acc:0.8721\n",
      "Training: epoch 65/70, batch 36/36, loss:0.0166, iou:0.6079, acc:0.9403\n",
      "Validation: batch 11/11, loss: 0.0388, iou: 0.4472, acc:0.8722\n",
      "Training: epoch 66/70, batch 36/36, loss:0.0167, iou:0.6055, acc:0.9399\n",
      "Validation: batch 11/11, loss: 0.0392, iou: 0.4417, acc:0.8715\n",
      "Training: epoch 67/70, batch 36/36, loss:0.0165, iou:0.6119, acc:0.9406\n",
      "Validation: batch 11/11, loss: 0.0373, iou: 0.4505, acc:0.8759\n",
      "Training: epoch 68/70, batch 36/36, loss:0.0162, iou:0.6126, acc:0.9415\n",
      "Validation: batch 11/11, loss: 0.0394, iou: 0.4433, acc:0.8712\n",
      "Training: epoch 69/70, batch 36/36, loss:0.0162, iou:0.6119, acc:0.9412\n",
      "Validation: batch 11/11, loss: 0.0395, iou: 0.4389, acc:0.8703\n",
      "Training: epoch 70/70, batch 36/36, loss:0.0161, iou:0.6117, acc:0.9416\n",
      "Validation: batch 11/11, loss: 0.0392, iou: 0.4494, acc:0.8697\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "valid_loss = []\n",
    "train_iou = []\n",
    "valid_iou = []\n",
    "train_acc = []\n",
    "valid_acc = []\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print('Training')\n",
    "\n",
    "for i in range(1,n_epochs+1):\n",
    "    train_gen = batch_gen(batch_size, img_files, mask_files, downsample)\n",
    "    \n",
    "    num_so_far = 0\n",
    "    for n, imgs, masks in train_gen:\n",
    "        _, t_loss, t_iou, t_acc = sess.run([train_step, loss, miou, macc], \n",
    "            {x: imgs, y: masks, is_train:True})\n",
    "        \n",
    "        train_loss.append(t_loss*len(imgs))\n",
    "        train_iou.append(t_iou*len(imgs))\n",
    "        train_acc.append(t_acc*len(imgs))\n",
    "        num_so_far += len(imgs)\n",
    "        mean_loss = np.sum(train_loss[-n:])/num_so_far\n",
    "        mean_iou = np.sum(train_iou[-n:])/num_so_far\n",
    "        mean_acc = np.sum(train_acc[-n:])/num_so_far\n",
    "        sys.stdout.write('\\rTraining: epoch {}/{}, batch {}/{}, loss:{:.4f}, iou:{:.4f}, acc:{:.4f}'.format(\n",
    "            i,n_epochs,n,batches_per_epoch, mean_loss, mean_iou, mean_acc))\n",
    "    \n",
    "    valid_gen = batch_gen(batch_size, valid_img_files, valid_mask_files, downsample)\n",
    "    \n",
    "    sys.stdout.write('\\r\\n')\n",
    "    \n",
    "    num_so_far = 0\n",
    "    \n",
    "    v_loss = 0\n",
    "    v_iou = 0\n",
    "    v_acc = 0\n",
    "    for n, imgs, masks in valid_gen:\n",
    "        v_l, v_i, v_a = sess.run([loss, miou, macc], {x: imgs, y: masks, is_train:False})\n",
    "        num_so_far += len(imgs)\n",
    "        v_loss += v_l*len(imgs)\n",
    "        v_iou += v_i*len(imgs)\n",
    "        v_acc += v_a*len(imgs)\n",
    "        \n",
    "        out_str = 'Validation: batch {}/{}'.format(n,num_valid_batches,n,batches_per_epoch)\n",
    "        sys.stdout.write('\\r{}'.format(out_str))\n",
    "        \n",
    "    mean_loss = v_loss/num_so_far\n",
    "    mean_iou = v_iou/num_so_far\n",
    "    mean_acc = v_acc/num_so_far\n",
    "    \n",
    "    valid_loss.append(mean_loss)\n",
    "    valid_iou.append(mean_iou)\n",
    "    valid_acc.append(mean_acc)\n",
    "    sys.stdout.write('\\r{}, loss: {:.4f}, iou: {:.4f}, acc:{:.4f}'.format(out_str, mean_loss, mean_iou, mean_acc))\n",
    "    sys.stdout.write('\\r\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "G    C    mIOU G     C   mIOU \n",
    "84.0 54.6 46.3 96.1 83.9 73.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
